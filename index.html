<!DOCTYPE HTML "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en">
  <head>
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
      /* Design Credits: Jon Barron, Deepak Pathak, Saurabh Gupta and Aditya Kusupati*/
      a {
        color: #1772d0;
        text-decoration: none;
      }
  
      a:focus,
      a:hover {
        color: #f09228;
        text-decoration: none;
      }
  
      body,
      td,
      th {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        font-size: 16px;
        font-weight: 400
      }
  
      heading {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        font-size: 19px;
        font-weight: 1000
      }
  
      strong {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        font-size: 16px;
        font-weight: 800
      }
  
      strongred {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        color: 'red';
        font-size: 16px
      }
  
      sectionheading {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        font-size: 22px;
        font-weight: 600
      }
  
      pageheading {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        font-size: 38px;
        font-weight: 400
      }
    </style>
    <!-- <link rel="icon" type="image/png" href="images/W.png"> -->
    <script type="text/javascript" src="js/hidebib.js"></script>
    <title>Shubhang Bhatnagar</title>

    <meta name="author" content="Shubhang Bhatnagar">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p class="name" style="text-align: center;">
                Shubhang Bhatnagar
              </p>
              <p>Hi, I am Shubhang Bhatnagar, a third year PhD student in the <a href="https://vision.ai.illinois.edu/">Computer Vision and Robotics Laboratory</a> at the <a href="https://illinois.edu/">University of Illinois Urbana-Champaign</a>. 
                I am advised by <a href="https://ece.illinois.edu/about/directory/faculty/n-ahuja"> Prof. Narendra Ahuja </a>. 
              </p>
              
              <p>
                Broadly, I am interested in computer vision, machine learning and their applications. Currently, my research focuses on semantic representation learning for robust, open-vocabulary recognition and retrieval.
              </p>
              <p>Previously, I completed my Dual degree (B.Tech + M.Tech) in electrical engineering from <a href="https://www.iitb.ac.in/">Indian Institute of Technology, Bombay</a>, where I was awarded the Institute Silver medal for graduating at the top of my batch. 
                In my Master's thesis, I worked on developing label efficient deep learning techniques using self-supervised and active learning advised by <a href="https://www.ee.iitb.ac.in/~asethi/">Prof. Amit Sethi</a>.
              </p>
              
              <p style="text-align:center">
                <a target="_blank" href="mailto:shubhangb97@gmail.com">E-mail</a> &nbsp;/&nbsp;
                 <a target="_blank" href="https://shubhangb97.github.io/files/Resume_Shubhang.pdf">Resume</a> &nbsp;/&nbsp;
                <a href="https://github.com/shubhangb97">GitHub</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=x_6C2vEAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/shubhang-bhatnagar-b78b24186"> LinkedIn </a> &nbsp;/&nbsp;
                <a href="https://twitter.com/s_bhatnagar_tw">Twitter</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/Web_img1.jpg">
            </td>
          </tr>
        </table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>Research</h2>
            
          </td>
        </tr>
      </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:40%;vertical-align:top;min-width:120px">
              <img src="images/ld_gr_iros.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:60%;vertical-align:top">
              <h3>Long-Distance Gesture Recognition using Dynamic Neural Networks</h3>
              <br>
              <strong>Shubhang Bhatnagar</strong>, Sharath Gopal, Narendra Ahuja, Liu Ren

              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS) </em>, 2023
              <br>
              <a href="javascript:toggleblock('ldgest_abs')">abstract</a> /
              <a href="https://shubhangb97.github.io/LD_GR_dynamic_nn">project page</a> /
              <a href="https://arxiv.org/abs/2308.04643">arxiv</a>
              <!-- <a href="coming soon">video</a> / 
               -->
              <br>

                <p align="justify"> <i id="ldgest_abs">Gestures form an important medium of communication between humans and machines. An overwhelming
                  majority of existing gesture recognition methods are tailored to
                  a scenario where humans and machines are located very close
                  to each other. This short-distance assumption does not hold
                  true for several types of interactions, for example gesture-based
                  interactions with a floor cleaning robot or with a drone. Methods
                  made for short-distance recognition are unable to perform
                  well on long-distance recognition due to gestures occupying
                  only a small portion of the input data. Their performance is
                  especially worse in resource constrained settings where they
                  are not able to effectively focus their limited compute on the
                  gesturing subject. We propose a novel, accurate and efficient
                  method for the recognition of gestures from longer distances. It
                  uses a dynamic neural network to select features from gesturecontaining
                  spatial regions of the input sensor data for further
                  processing. This helps the network focus on features important
                  for gesture recognition while discarding background features
                  early on, thus making it more compute efficient compared
                  to other techniques. We demonstrate the performance of our
                  method on the LD-ConGR long-distance dataset where it
                  outperforms previous state-of-the-art methods on recognition
                  accuracy and compute efficiency.</i></p>
              
            </td>
          </tr>



          <tr>
            <td style="padding:2.5%;width:40%;vertical-align:top;min-width:120px">
              <img src="images/PAL_bmvc.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:60%;vertical-align:top">
              <h3>PAL: Pretext based Active Learning</h3>
              <br>
              <strong>Shubhang Bhatnagar</strong>, Sachin Goyal<sup>*</sup>, Darshan Tank<sup>*</sup>, Amit Sethi
              <br><strong>British Machine Vision Conference (BMVC), 2021</strong>
              <br>
             
              <a href="javascript:toggleblock('pal_abs')">abstract</a> /
              <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_1061.html">project page</a> /
              <a href="https://www.bmvc2021-virtualconference.com/assets/papers/1061.pdf">paper</a> /
              <a href="https://github.com/shubhangb97/PAL_pretext_based_active_learning">code</a> 
              <br>

                <p align="justify"> <i id="pal_abs">The goal of pool-based active learning is to judiciously select a fixed-sized subset of
                  unlabeled samples from a pool to query an oracle for their labels, in order to maximize
                  the accuracy of a supervised learner. However, the unsaid requirement that the oracle
                  should always assign correct labels is unreasonable for most situations. We propose an
                  active learning technique for deep neural networks that is more robust to mislabeling than
                  the previously proposed techniques. Previous techniques rely on the task network itself
                  to estimate the novelty of the unlabeled samples, but learning the task (generalization)
                  and selecting samples (out-of-distribution detection) can be conflicting goals. We use
                  a separate network to score the unlabeled samples for selection. The scoring network
                  relies on self-supervision for modeling the distribution of the labeled samples to reduce
                  the dependency on potentially noisy labels. To counter the paucity of data, we also deploy
                  another head on the scoring network for regularization via multi-task learning and use an
                  unusual self-balancing hybrid scoring function. Furthermore, we divide each query into
                  sub-queries before labeling to ensure that the query has diverse samples. In addition to
                  having a higher tolerance to mislabeling of samples by the oracle, the resultant technique
                  also produces competitive accuracy in the absence of label noise. The technique also
                  handles the introduction of new classes on-the-fly well by temporarily increasing the
                  sampling rate of these classes. We make our code publicly available at https://
                  github.com/shubhangb97/PAL_pretext_based_active_learning</i></p>
              
            </td>
          </tr>


          <td style="padding:2.5%;width:40%;vertical-align:top;min-width:120px">
            <img src="images/l1_eusipco.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
          </td>
          <td style="padding:2.5%;width:60%;vertical-align:top">
            <h3>Analyzing Cross Validation in Compressed Sensing with Mixed Gaussian and Impulse Measurement Noise with L1 Errors</h3>
            <br>
            <strong>Shubhang Bhatnagar</strong><sup>*</sup>, Chinmay Gurjarpadhye<sup>*</sup>, Ajit Rajwade
            <br><strong>European Signal Processing Conference (EUSIPCO),</strong>
            <br>
            <a href="javascript:toggleblock('l1_cv_abs')">abstract</a> /
            <a href="https://ieeexplore.ieee.org/document/9615951">paper</a> /
            <a href="https://arxiv.org/abs/2102.10165">extended arxiv</a>
            
            <br>

              <p align="justify"> <i id="l1_cv_abs">Compressed sensing (CS) involves sampling 
                signals at rates less than their Nyquist rates and attempting to reconstruct them after sample acquisition. 
                Most such algorithms have parameters, for example the regularization parameter in LASSO, which need to be chosen carefully for optimal 
                performance. These parameters can be chosen based on assumptions on the noise level or signal sparsity, but this knowledge may often be 
                unavailable. In such cases, cross validation (CV) can be used to choose these parameters in a purely data-driven fashion. Previous work 
                analyzing the use of CV in CS has been based on the ‚Ñì2 cross-validation error with Gaussian measurement noise. But it is well known that the ‚Ñì2 
                error is not robust to impulse noise and provides a poor estimate of the recovery error, failing to choose the best parameter. Here we propose 
                using the ‚Ñì1‚àíCV error which provides substantial performance benefits given impulse measurement noise. Most importantly, we provide a detailed 
                theoretical analysis and error bounds for the use of ‚Ñì1‚àíCV error in CS reconstruction. We show that with high probability, choosing the parameter
                 that yields the minimum ‚Ñì1‚àíCV error is equivalent to choosing the minimum recovery error (which is not observable in practice). To our best 
                 knowledge, this is the first paper which theoretically analyzes ‚Ñì1 -based CV in CS.</i></p>
            
          </td>
        </tr>

        <td style="padding:2.5%;width:40%;vertical-align:top;min-width:120px">
          <img src="images/subband_coder.PNG" alt="project image" style="width:auto; height:auto; max-width:100%;" />
        </td>
        <td style="padding:2.5%;width:60%;vertical-align:top">
          <h3>Insights on coding gain and its properties for principal component filter banks</h3>
          <br>
          Prasad Chaphekar, Aniket Bhatia,<strong>Shubhang Bhatnagar</strong>, Abhiraj Kanse, Ashish V Vanmali, Vikram M Gadre
          <br><strong> SƒÅdhanƒÅ</strong> , Journal of the Indian Academy of Sciences, 2023
          <br>
          <a href="javascript:toggleblock('pcfb_abs')">abstract</a> /
          <a href="https://rdcu.be/dbulY">paper</a>
                   
          <br>

            <p align="justify"> <i id="pcfb_abs">Principal Component Filter Bank (PCFB) is considered optimal in terms of coding gain for speciÔ¨Åcconditions. 
              P P Vaidyanathan stated that coding gain does not necessarily always increase with the increase inthe number of bands. However, very few attempts are made 
              in the literature to go beyond the conÔ¨Ånes of work done by P P Vaidyanathan. We present analytic proofs for the monotonicity of speciÔ¨Åc shapes of PSDs.
               This papers also derives properties of coding gain of PCFBs, which brings the new insights on the coding gain of Principal Component Filter Banks.</i></p>
          
        </td>
      </tr>



      <td style="padding:2.5%;width:40%;vertical-align:top;min-width:120px">
        <img src="images/qr_hopfield_denoise.PNG" alt="project image" style="width:auto; height:auto; max-width:100%;" />
      </td>
      <td style="padding:2.5%;width:60%;vertical-align:top">
        <h3>QR Code Denoising using Parallel Hopfield Networks</h3>
        <br>
        <strong>Shubhang Bhatnagar</strong><sup>*</sup>, Ishan Bhatnagar<sup>*</sup>
        <br><strong>Arxiv</strong> , 2018
        <br>
        <a href="javascript:toggleblock('qr_abs')">abstract</a> /
        <a href="https://arxiv.org/abs/1812.01065">arxiv</a>
                 
        <br>

          <p align="justify"> <i id="qr_abs">We propose a novel algorithm for using Hopfield
            networks to denoise QR codes. Hopfield networks have mostly been
            used as a noise tolerant memory or to solve difficult combinatorial
            problems. One of the major drawbacks in their use in noise tolerant
            associative memory is their low capacity of storage, scaling only
            linearly with the number of nodes in the network. A larger capacity
            therefore requires a larger number of nodes, thereby reducing the
            speed of convergence of the network in addition to increasing
            hardware costs for acquiring more precise data to be fed to a larger
            number of nodes. Our paper proposes a new algorithm to allow the
            use of several Hopfield networks in parallel thereby increasing the
            cumulative storage capacity of the system many times as compared
            to a single Hopfield network. Our algorithm would also be much
            faster than a larger single Hopfield network with the same total
            capacity. This enables their use in applications like denoising QR
            codes, which we have demonstrated in our paper. We then test our
            network on a large set of QR code images with different types of
            noise and demonstrate that such a system of Hopfield networks can
            be used to denoise and recognize QR codes in real time.</i></p>
        
      </td>
    </tr>


    <td style="padding:2.5%;width:40%;vertical-align:top;min-width:120px">
      <img src="images/adapter_img.PNG" alt="project image" style="width:auto; height:auto; max-width:100%;" />
    </td>
    <td style="padding:2.5%;width:60%;vertical-align:top">
      <h3>Memory Efficient Adaptive Attention For Multiple Domain Learning</h3>
      <br>
      Himanshu Pradeep Aswani, Abhiraj Sunil Kanse, <strong>Shubhang Bhatnagar</strong>, Amit Sethi
      <br><strong>Arxiv</strong> , 2021
      <br>
      <a href="javascript:toggleblock('adapter_abs')">abstract</a> /
      <a href="https://arxiv.org/abs/2110.10969">arxiv</a>
               
      <br>

        <p align="justify"> <i id="adapter_abs">We propose a novel algorithm for using Hopfield
          networks to denoise QR codes. Hopfield networks have mostly been
          used as a noise tolerant memory or to solve difficult combinatorial
          problems. One of the major drawbacks in their use in noise tolerant
          associative memory is their low capacity of storage, scaling only
          linearly with the number of nodes in the network. A larger capacity
          therefore requires a larger number of nodes, thereby reducing the
          speed of convergence of the network in addition to increasing
          hardware costs for acquiring more precise data to be fed to a larger
          number of nodes. Our paper proposes a new algorithm to allow the
          use of several Hopfield networks in parallel thereby increasing the
          cumulative storage capacity of the system many times as compared
          to a single Hopfield network. Our algorithm would also be much
          faster than a larger single Hopfield network with the same total
          capacity. This enables their use in applications like denoising QR
          codes, which we have demonstrated in our paper. We then test our
          network on a large set of QR code images with different types of
          noise and demonstrate that such a system of Hopfield networks can
          be used to denoise and recognize QR codes in real time.</i></p>
      
    </td>
  </tr>


        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td><br>
              <p align="right">
                <font size="2">
                  Template: <a href="https://jonbarron.info">this</a>
                </font>
              </p>
            </td>
          </tr>
        </table>
        <script xml:space="preserve" language="JavaScript">
          hideblock('ldgest_abs');
        </script>
         <script xml:space="preserve" language="JavaScript">
          hideblock('l1_cv_abs');
        </script>
         <script xml:space="preserve" language="JavaScript">
          hideblock('pal_abs');
        </script>
        <script xml:space="preserve" language="JavaScript">
          hideblock('pcfb_abs');
        </script>
        <script xml:space="preserve" language="JavaScript">
          hideblock('qr_abs');
        </script>
        <script xml:space="preserve" language="JavaScript">
          hideblock('adapter_abs');
        </script>
        <br>
        <br>
        <br>
        <!--
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Intel Projects</h2>
              <p>Besides my work on the RealSense depth sensors and the publications above, a sampling of my publicly disclosed work
              </p>
            </td>
          </tr>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

          {% for post in site.posts %}
          {% for cat in post.categories %}
          {% if cat == 'Intel' %}
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/tn{{post.image}}" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>{{post.title}}</h3>
              <br>
              <em>{{post.categories}} {{post.course}}</em>
              <br>
              {{ post.date | date: "%Y-%m-%d" }}

              <br>
              {% if post.website %}
              <a href="{{post.website}}">website</a> /
              {% endif %}
              {% if post.paper %}
              <a href="{{post.paper}}">paper</a> /
              {% endif %}
              {% if post.patent %}
              <a href="{{post.patent}}">patent</a> /
              {% endif %}
              {% if post.patent2 %}
              <a href="{{post.patent2}}">patent #2</a> /
              {% endif %}
              {% if post.patent3 %}
              <a href="{{post.patent3}}">patent #3</a> /
              {% endif %}
              {% if post.video %}
              <a href="{{post.video}}">video</a> /
              {% endif %}
              {% if post.video2 %}
              <a href="{{post.video2}}">video #2</a> /
              {% endif %}
              {% if post.code %}
              <a href="{{post.code}}">code</a> /
              {% endif %}
              {% if post.poster %}
              <a href="{{post.poster}}">poster</a> /
              {% endif %}
              {% if post.slides %}
              <a href="{{post.slides}}">slides</a> /
              {% endif %}
              <p></p>
              {{ post.excerpt }}
            </td>
          </tr>
          {% endif %}
          {% endfor %}
          {% endfor %}
        </table>-->
        
</body>

</html>

